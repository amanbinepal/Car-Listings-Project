{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Counts before removing outliers and na values:\n",
      "price           426853\n",
      "manufacturer    409234\n",
      "odometer        422480\n",
      "year            425675\n",
      "dtype: int64\n",
      "The average odometer reading is: 90087.63958015172\n",
      "Cleaned average odometer reading: 87101.48424280442\n",
      "Cleaned average price: 18657.11817339443\n",
      "Cleaned average year: 2012.9153152235176\n",
      "\n",
      "Counts after removing outliers and na values:\n",
      "price           314047\n",
      "manufacturer    314047\n",
      "odometer        314047\n",
      "year            314047\n",
      "type            314047\n",
      "dtype: int64\n",
      "\n",
      "Counts of NA values for 'price', 'year', 'odometer', 'manufacturer', 'type' before dropping missing values and outliers:\n",
      "price               0\n",
      "year             1178\n",
      "odometer         4373\n",
      "manufacturer    17619\n",
      "type            92831\n",
      "dtype: int64\n",
      "\n",
      "Counts of NA values for 'price', 'year', 'odometer', 'manufacturer', 'type' after dropping missing values and outliers:\n",
      "price           0\n",
      "year            0\n",
      "odometer        0\n",
      "manufacturer    0\n",
      "type            0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import zscore\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the dataset into a pandas DataFrame\n",
    "df = pd.read_csv('car_listings.csv')\n",
    "\n",
    "print(\"\\nCounts before removing outliers and na values:\")\n",
    "print(df[['price', 'manufacturer', 'odometer', 'year']].count())\n",
    "\n",
    "\n",
    "# Drop rows with missing values in the 'price', 'manufacturer', 'odometer', and 'year' columns\n",
    "#df_filtered = df.dropna(subset=['price', 'manufacturer', 'odometer', 'year'])\n",
    "df_filtered = df.dropna(subset=['price', 'manufacturer', 'odometer', 'year', 'type'])\n",
    "\n",
    "\n",
    "# Calculate the mean of the odometer readings\n",
    "average_odometer_mean = df_filtered['odometer'].mean()\n",
    "print(f\"The average odometer reading is: {average_odometer_mean}\")\n",
    "\n",
    "# Remove outliers for 'price' based on the z-score\n",
    "# Calculate z-scores of 'price'\n",
    "z_scores_price = np.abs(zscore(df_filtered['price']))\n",
    "# Keep rows with z-scores less than 3\n",
    "df_filtered = df_filtered[(z_scores_price < 3)]\n",
    "\n",
    "# Remove negative prices, if any\n",
    "df_filtered = df_filtered[df_filtered['price'] >= 0]\n",
    "\n",
    "# Perform the same outlier removal for 'odometer' and 'year'\n",
    "# Calculate z-scores for 'odometer' and 'year'\n",
    "z_scores_odometer = np.abs(zscore(df_filtered['odometer']))\n",
    "z_scores_year = np.abs(zscore(df_filtered['year']))\n",
    "\n",
    "# Filter out the outliers\n",
    "df_filtered = df_filtered[(z_scores_odometer < 3) & (z_scores_year < 3)]\n",
    "\n",
    "# After removing outliers, you may want to check the mean values again\n",
    "average_odometer_mean = df_filtered['odometer'].mean()\n",
    "average_price_mean = df_filtered['price'].mean()\n",
    "average_year_mean = df_filtered['year'].mean()\n",
    "\n",
    "# Print out the cleaned mean values\n",
    "print(f\"Cleaned average odometer reading: {average_odometer_mean}\")\n",
    "print(f\"Cleaned average price: {average_price_mean}\")\n",
    "print(f\"Cleaned average year: {average_year_mean}\")\n",
    "\n",
    "# Display counts after removing outliers\n",
    "print(\"\\nCounts after removing outliers and na values:\")\n",
    "print(df_filtered[['price', 'manufacturer', 'odometer', 'year', 'type']].count())\n",
    "\n",
    "print(\"\\nCounts of NA values for 'price', 'year', 'odometer', 'manufacturer', 'type' before dropping missing values and outliers:\")\n",
    "print(df[['price', 'year', 'odometer', 'manufacturer', 'type']].isna().sum())\n",
    "\n",
    "\n",
    "print(\"\\nCounts of NA values for 'price', 'year', 'odometer', 'manufacturer', 'type' after dropping missing values and outliers:\")\n",
    "print(df_filtered[['price', 'year', 'odometer', 'manufacturer', 'type']].isna().sum())\n",
    "\n",
    "# Encode the 'manufacturer' target variable\n",
    "#label_encoder = LabelEncoder()\n",
    "#y = label_encoder.fit_transform(df_filtered['manufacturer'])\n",
    "\n",
    "# Apply one-hot encoding to categorical features including 'type'\n",
    "#X = pd.get_dummies(df_filtered.drop(['manufacturer'], axis=1))\n",
    "\n",
    "# Ready for further analysis or model training\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "df_filtered['type'] = label_encoder.fit_transform(df_filtered['type'])\n",
    "\n",
    "# The data is now cleaned and ready for further analysis or model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df_filtered[['price', 'year', 'odometer', 'type']]\n",
    "target = df_filtered['manufacturer']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "feature_train, feature_test, target_train, target_test = train_test_split(features, target, test_size=0.3, random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\binep\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:757: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "c:\\Users\\binep\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:595: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "c:\\Users\\binep\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:604: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "c:\\Users\\binep\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:757: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "c:\\Users\\binep\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:595: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "c:\\Users\\binep\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:604: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "c:\\Users\\binep\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:757: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "c:\\Users\\binep\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:595: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "c:\\Users\\binep\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:604: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "feature_train = sc.fit_transform(feature_train)\n",
    "feature_test = sc.transform(feature_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predictions:\n",
      "['ford' 'ford' 'ford' ... 'ford' 'ford' 'ford']\n",
      "Accuracy: 0.16540890516372128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\binep\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "logreg = LogisticRegression(solver='lbfgs')\n",
    "logreg.fit(feature_train, target_train)\n",
    "\n",
    "predictions = logreg.predict(feature_test)\n",
    "\n",
    "print(\"\\nPredictions:\")\n",
    "print(predictions)\n",
    "\n",
    "accuracy = accuracy_score(target_test, predictions)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['chevrolet' 'ford' 'volkswagen' ... 'subaru' 'lexus' 'ram']\n",
      "Accuracy: 0.6258239133895876\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "decision_tree = DecisionTreeClassifier(random_state=1)\n",
    "decision_tree.fit(feature_train, target_train)\n",
    "\n",
    "predictions = decision_tree.predict(feature_test)\n",
    "accuracy = accuracy_score(target_test, predictions)\n",
    "print(predictions)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['chevrolet' 'ford' 'volkswagen' ... 'subaru' 'honda' 'ram']\n",
      "Accuracy: 0.6412354720585894\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "random_forest = RandomForestClassifier(random_state=1)\n",
    "random_forest.fit(feature_train, target_train)\n",
    "\n",
    "predictions = random_forest.predict(feature_test)\n",
    "accuracy = accuracy_score(target_test, predictions)\n",
    "print(predictions)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\binep\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:757: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "c:\\Users\\binep\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:595: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "c:\\Users\\binep\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:604: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "c:\\Users\\binep\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:757: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "c:\\Users\\binep\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:595: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "c:\\Users\\binep\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:604: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "c:\\Users\\binep\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:757: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "c:\\Users\\binep\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:595: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "c:\\Users\\binep\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:604: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "c:\\Users\\binep\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_split.py:684: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
      "  warnings.warn(\n",
      "c:\\Users\\binep\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\Users\\binep\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\binep\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\Users\\binep\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\Users\\binep\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\binep\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\Users\\binep\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\binep\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\Users\\binep\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\binep\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\Users\\binep\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\binep\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\binep\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\binep\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\Users\\binep\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\binep\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\Users\\binep\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\Users\\binep\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\binep\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\Users\\binep\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\binep\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\Users\\binep\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\binep\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\Users\\binep\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\binep\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\binep\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\binep\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\Users\\binep\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\binep\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\Users\\binep\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\Users\\binep\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\binep\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\Users\\binep\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\binep\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\Users\\binep\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\binep\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\Users\\binep\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Usage Increase: -183.61328125 MB\n",
      "Best Parameters:  {'C': 0.01, 'penalty': 'l2', 'solver': 'saga'}\n",
      "Best Score:  0.1736796208873523\n",
      "Grid Search Test Accuracy:  0.1686478454680535\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import psutil\n",
    "\n",
    "# Logistic Regression\n",
    "\n",
    "# Assuming your dataset is a pandas DataFrame 'df'\n",
    "sample_df = df_filtered.sample(frac=0.1, random_state=0)  # Sampling 10% of the data\n",
    "\n",
    "# Now split this sampled data into features and target, and then into training and test sets\n",
    "feature_sample = sample_df[['price', 'year', 'odometer', 'type']]\n",
    "target_sample = sample_df['manufacturer']\n",
    "\n",
    "feature_train_sample, feature_test_sample, target_train_sample, target_test_sample = train_test_split(feature_sample, target_sample, test_size=0.3, random_state=0)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "feature_train_sample = sc.fit_transform(feature_train_sample)\n",
    "feature_test_sample = sc.transform(feature_test_sample)\n",
    "\n",
    "param_grid = {\n",
    "    #'C': [0.01, 0.1, 1, 10, 100],\n",
    "    'C': [0.01, 0.1, 10],\n",
    "    'penalty': ['l2', 'none'],\n",
    "    'solver': ['lbfgs', 'saga']\n",
    "    #'C': [0.1, 1, 10],\n",
    "    #'penalty': ['l2'],\n",
    "}\n",
    "\n",
    "before_memory = psutil.virtual_memory().used / (1024 ** 2)\n",
    "\n",
    "#grid_search = GridSearchCV(LogisticRegression(solver='lbfgs', max_iter=1000), param_grid, cv=5, scoring='accuracy')\n",
    "grid_search = GridSearchCV(LogisticRegression(max_iter=500), param_grid, cv=3, scoring='accuracy')\n",
    "grid_search.fit(feature_train_sample, target_train_sample)\n",
    "\n",
    "after_memory = psutil.virtual_memory().used / (1024 ** 2)  # Convert to MB\n",
    "\n",
    "# Calculate memory usage increase\n",
    "memory_increase = after_memory - before_memory\n",
    "\n",
    "print(f\"Memory Usage Increase: {memory_increase} MB\")\n",
    "\n",
    "print(\"Best Parameters: \", grid_search.best_params_)\n",
    "print(\"Best Score: \", grid_search.best_score_)\n",
    "\n",
    "best_grid = grid_search.best_estimator_\n",
    "grid_predictions = best_grid.predict(feature_test_sample)\n",
    "print(\"Grid Search Test Accuracy: \", accuracy_score(target_test_sample, grid_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\binep\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_split.py:684: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
      "  warnings.warn(\n",
      "c:\\Users\\binep\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\Users\\binep\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\binep\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\Users\\binep\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\Users\\binep\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\binep\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\Users\\binep\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\binep\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\Users\\binep\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\binep\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\Users\\binep\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\binep\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\Users\\binep\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\binep\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\Users\\binep\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\Users\\binep\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\binep\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\Users\\binep\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\binep\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\Users\\binep\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "c:\\Users\\binep\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Usage Increase: 800.80859375 MB\n",
      "Best Parameters:  {'solver': 'lbfgs', 'penalty': 'l2', 'C': 0.1}\n",
      "Best Score:  0.17290639951256037\n",
      "Randomized Search Test Accuracy:  0.17013372956909362\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "#randomized_search = RandomizedSearchCV(LogisticRegression(solver='lbfgs', max_iter=1000), param_distributions=param_grid, n_iter=5, cv=3, scoring='accuracy', random_state=0)\n",
    "before_memory = psutil.virtual_memory().used / (1024 ** 2)\n",
    "\n",
    "\n",
    "randomized_search = RandomizedSearchCV(LogisticRegression(max_iter=500), param_distributions=param_grid, n_iter=5, cv=3, scoring='accuracy', random_state=0)\n",
    "randomized_search.fit(feature_train_sample, target_train_sample)\n",
    "\n",
    "after_memory = psutil.virtual_memory().used / (1024 ** 2)  # Convert to MB\n",
    "\n",
    "# Calculate memory usage increase\n",
    "memory_increase = after_memory - before_memory\n",
    "\n",
    "print(f\"Memory Usage Increase: {memory_increase} MB\")\n",
    "\n",
    "print(\"Best Parameters: \", randomized_search.best_params_)\n",
    "print(\"Best Score: \", randomized_search.best_score_)\n",
    "\n",
    "best_random = randomized_search.best_estimator_\n",
    "random_predictions = best_random.predict(feature_test_sample)\n",
    "print(\"Randomized Search Test Accuracy: \", accuracy_score(target_test_sample, random_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\binep\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_split.py:684: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Usage Increase: -69.75 MB\n",
      "Decision Tree Best Parameters:  {'max_depth': 30, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "Decision Tree Best Score:  0.3375335114540624\n",
      "Decision Tree Test Accuracy:  0.3593716832944173\n"
     ]
    }
   ],
   "source": [
    "param_grid_dt = {\n",
    "    'max_depth': [None, 10, 20, 30, 40, 50],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "before_memory = psutil.virtual_memory().used / (1024 ** 2)\n",
    "\n",
    "grid_search_dt = GridSearchCV(DecisionTreeClassifier(random_state=0), param_grid_dt, cv=5, scoring='accuracy')\n",
    "grid_search_dt.fit(feature_train_sample, target_train_sample)\n",
    "\n",
    "after_memory = psutil.virtual_memory().used / (1024 ** 2)  # Convert to MB\n",
    "\n",
    "# Calculate memory usage increase\n",
    "memory_increase = after_memory - before_memory\n",
    "\n",
    "\n",
    "print(f\"Memory Usage Increase: {memory_increase} MB\")\n",
    "\n",
    "print(\"Decision Tree Best Parameters: \", grid_search_dt.best_params_)\n",
    "print(\"Decision Tree Best Score: \", grid_search_dt.best_score_)\n",
    "\n",
    "best_dt = grid_search_dt.best_estimator_\n",
    "dt_predictions = best_dt.predict(feature_test_sample)\n",
    "print(\"Decision Tree Test Accuracy: \", accuracy_score(target_test_sample, dt_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\binep\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_split.py:684: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Usage Increase: 17.3984375 MB\n",
      "Decision Tree Best Parameters:  {'min_samples_split': 2, 'min_samples_leaf': 1, 'max_depth': 30}\n",
      "Decision Tree Best Score:  0.3375335114540624\n",
      "Decision Tree (Randomized Search) Test Accuracy:  0.3593716832944173\n"
     ]
    }
   ],
   "source": [
    "before_memory = psutil.virtual_memory().used / (1024 ** 2)\n",
    "\n",
    "randomized_search_dt = RandomizedSearchCV(DecisionTreeClassifier(random_state=0), param_distributions=param_grid_dt, n_iter=20, cv=5, scoring='accuracy', random_state=0)\n",
    "randomized_search_dt.fit(feature_train_sample, target_train_sample)\n",
    "\n",
    "after_memory = psutil.virtual_memory().used / (1024 ** 2)  # Convert to MB\n",
    "\n",
    "# Calculate memory usage increase\n",
    "memory_increase = after_memory - before_memory\n",
    "\n",
    "print(f\"Memory Usage Increase: {memory_increase} MB\")\n",
    "\n",
    "# Print the best parameters found by Randomized Search\n",
    "print(\"Decision Tree Best Parameters: \", randomized_search_dt.best_params_)\n",
    "print(\"Decision Tree Best Score: \", randomized_search_dt.best_score_)\n",
    "\n",
    "# Use the best estimator to make predictions\n",
    "best_dt_random = randomized_search_dt.best_estimator_\n",
    "dt_random_predictions = best_dt_random.predict(feature_test_sample)\n",
    "\n",
    "# Evaluate the predictions\n",
    "print(\"Decision Tree (Randomized Search) Test Accuracy: \", accuracy_score(target_test_sample, dt_random_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\binep\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_split.py:684: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Usage Increase: -36.98046875 MB\n",
      "Random Forest Best Parameters:  {'max_depth': 20, 'min_samples_split': 5, 'n_estimators': 100}\n",
      "Random Forest Best Score:  0.3570941776035635\n",
      "Random Forest Test Accuracy:  0.38017406070897897\n"
     ]
    }
   ],
   "source": [
    "param_grid_rf = {\n",
    "    'n_estimators': [10, 50 ,100],\n",
    "    #'max_depth': [None, 20],\n",
    "    'max_depth': [None, 10, 20, 30, 40, 50],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "before_memory = psutil.virtual_memory().used / (1024 ** 2)\n",
    "\n",
    "grid_search_rf = GridSearchCV(RandomForestClassifier(random_state=0), param_grid_rf, cv=3, scoring='accuracy')\n",
    "grid_search_rf.fit(feature_train_sample, target_train_sample)\n",
    "\n",
    "after_memory = psutil.virtual_memory().used / (1024 ** 2)  # Convert to MB\n",
    "\n",
    "# Calculate memory usage increase\n",
    "memory_increase = after_memory - before_memory\n",
    "\n",
    "\n",
    "print(f\"Memory Usage Increase: {memory_increase} MB\")\n",
    "\n",
    "print(\"Random Forest Best Parameters: \", grid_search_rf.best_params_)\n",
    "print(\"Random Forest Best Score: \", grid_search_rf.best_score_)\n",
    "\n",
    "best_rf = grid_search_rf.best_estimator_\n",
    "rf_predictions = best_rf.predict(feature_test_sample)\n",
    "print(\"Random Forest Test Accuracy: \", accuracy_score(target_test_sample, rf_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\binep\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_split.py:684: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Usage Increase: 289.34765625 MB\n",
      "Random Forest Best Parameters:  {'n_estimators': 100, 'min_samples_split': 5, 'max_depth': 30}\n",
      "Random Forest Best Score:  0.3549561722023129\n",
      "Random Forest (Randomized Search) Test Accuracy:  0.38410104011887075\n"
     ]
    }
   ],
   "source": [
    "before_memory = psutil.virtual_memory().used / (1024 ** 2)\n",
    "\n",
    "randomized_search_rf = RandomizedSearchCV(RandomForestClassifier(random_state=0), param_distributions=param_grid_rf, n_iter=20, cv=3, scoring='accuracy', random_state=0)\n",
    "randomized_search_rf.fit(feature_train_sample, target_train_sample)\n",
    "\n",
    "after_memory = psutil.virtual_memory().used / (1024 ** 2)  # Convert to MB\n",
    "\n",
    "memory_increase = after_memory - before_memory\n",
    "\n",
    "print(f\"Memory Usage Increase: {memory_increase} MB\")\n",
    "\n",
    "# Print the best parameters found by Randomized Search\n",
    "print(\"Random Forest Best Parameters: \", randomized_search_rf.best_params_)\n",
    "print(\"Random Forest Best Score: \", randomized_search_rf.best_score_)\n",
    "\n",
    "# Use the best estimator to make predictions\n",
    "best_rf_random = randomized_search_rf.best_estimator_\n",
    "rf_random_predictions = best_rf_random.predict(feature_test_sample)\n",
    "\n",
    "# Evaluate the predictions\n",
    "print(\"Random Forest (Randomized Search) Test Accuracy: \", accuracy_score(target_test_sample, rf_random_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\binep\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\binep\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Logistic Regression\n",
      "Accuracy: 0.16540890516372128\n",
      "Precision: 0.08048669033681075\n",
      "Recall: 0.16540890516372128\n",
      "F1-Score: 0.07483030341995761\n",
      "Confusion Matrix:\n",
      " [[ 0  0  0 ...  0  1  0]\n",
      " [ 0  0  0 ...  0  0  0]\n",
      " [ 0  0  0 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ...  0 10  0]\n",
      " [ 0  0  0 ...  0  6  0]\n",
      " [ 0  0  0 ...  0  1  0]]\n",
      "\n",
      "\n",
      "Model: Decision Tree\n",
      "Accuracy: 0.6258239133895876\n",
      "Precision: 0.6260713171156923\n",
      "Recall: 0.6258239133895876\n",
      "F1-Score: 0.625893204100852\n",
      "Confusion Matrix:\n",
      " [[1115    3    0 ...   75    8    9]\n",
      " [   1  210    0 ...    0    1    1]\n",
      " [   0    0    2 ...    0    0    0]\n",
      " ...\n",
      " [  60    0    0 ... 4898  102   31]\n",
      " [   6    1    0 ...   88 1363    8]\n",
      " [   6    1    0 ...   26   13  512]]\n",
      "\n",
      "\n",
      "Model: Random Forest\n",
      "Accuracy: 0.6412354720585894\n",
      "Precision: 0.6422372449535945\n",
      "Recall: 0.6412354720585894\n",
      "F1-Score: 0.6410339120880725\n",
      "Confusion Matrix:\n",
      " [[1117    3    0 ...   79    6    9]\n",
      " [   0  211    0 ...    0    2    1]\n",
      " [   0    0    1 ...    0    0    0]\n",
      " ...\n",
      " [  43    1    0 ... 5031   62   27]\n",
      " [   9    1    0 ...  100 1359    6]\n",
      " [   2    0    0 ...   36   11  508]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "def evaluate_classification_model(model_name, model, X_train, y_train, X_test, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    \n",
    "    # Basic Metrics\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    precision = precision_score(y_test, predictions, average='weighted')\n",
    "    recall = recall_score(y_test, predictions, average='weighted')\n",
    "    f1 = f1_score(y_test, predictions, average='weighted')\n",
    "    cm = confusion_matrix(y_test, predictions)\n",
    "\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"F1-Score:\", f1)\n",
    "    print(\"Confusion Matrix:\\n\", cm)\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "evaluate_classification_model(\"Logistic Regression\", logreg, feature_train, target_train, feature_test, target_test)\n",
    "evaluate_classification_model(\"Decision Tree\", decision_tree, feature_train, target_train, feature_test, target_test)\n",
    "evaluate_classification_model(\"Random Forest\", random_forest, feature_train, target_train, feature_test, target_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
